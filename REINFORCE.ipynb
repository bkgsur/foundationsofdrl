{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "REINFORCE.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNev7RvbvYU6V50SQmxeflu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bkgsur/foundationsofdrl/blob/main/REINFORCE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3MRMN9uFhF3"
      },
      "source": [
        "from torch.distributions import Categorical\r\n",
        "import gym\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BArbl4ocGdPp"
      },
      "source": [
        "#Constants\r\n",
        "GAMMA = 0.99\r\n",
        "LEARNING_RATE= 0.01\r\n",
        "HIDDEN_UNITS= 64\r\n",
        "EPISODES = 300\r\n",
        "TIME_UNITS=200\r\n",
        "TERMINATION = 195"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3PG9qk2Gh2x"
      },
      "source": [
        "class Policy(nn.Module):\r\n",
        "  def __init__(self, in_dim,out_dim):\r\n",
        "    super(Policy,self).__init__()\r\n",
        "    layers = [nn.Linear(in_dim,HIDDEN_UNITS),nn.ReLU(),nn.Linear(HIDDEN_UNITS,out_dim)]\r\n",
        "    self.policynetwork = nn.Sequential(*layers)\r\n",
        "    self.onpolicy_reset()\r\n",
        "    self.train()\r\n",
        "\r\n",
        "  def onpolicy_reset(self):\r\n",
        "    self.log_probablity_actions = []\r\n",
        "    self.rewards = []\r\n",
        "\r\n",
        "  def action_based_on_state(self, state):     \r\n",
        "    policydistribution  = Categorical(logits = self.model(torch.from_numpy(state.astype(np.float32)))) # policy distibution based on state  - pi(a|s) action based on states  \r\n",
        "    action = policydistribution.sample()# sample action from policy  \r\n",
        "    return action.item(),policydistribution.log_prob(action)   \r\n",
        "\r\n",
        "def train_per_episode(policy,optimizer): \r\n",
        "  returns =  np.empty(TIME_UNITS,dtype=np.float32)\r\n",
        "  future_return=0.0\r\n",
        "  for t in reversed(range(TIME_UNITS)):\r\n",
        "    future_return = policy.rewards[t] +  GAMMA * future_return\r\n",
        "    returs[t] = future_return\r\n",
        "  returns = torch.tensor(returns)\r\n",
        "  log_probablity_actions = torch.stack(policy.log_probablity_actions)\r\n",
        "  loss = torch.sum(-log_probablity_actions*returns)\r\n",
        "  optimizer.zero_grad()\r\n",
        "  loss.backward()\r\n",
        "  optimizer.step()\r\n",
        "  return loss\r\n",
        "\r\n",
        "  def main():\r\n",
        "    env = gym.make(\"CartPole-v0\")\r\n",
        "    in_dim = env.observation_space.shape[0]\r\n",
        "    out_dim = env.action_space.n\r\n",
        "    policy = Policy(in_dim,out_dim)\r\n",
        "    optimizer = optim.Adam(policy.parameters(), lr= LEARNING_RATE)\r\n",
        "\r\n",
        "    for epi in range(EPISODES):\r\n",
        "      state = env.reset()\r\n",
        "      for t in range(TIME_UNITS):\r\n",
        "        action,log_probablity_action = policy.act(state)\r\n",
        "        state,reward,done,_ = env.step(action)\r\n",
        "        policy.log_probablity_actions.Add(log_probablity_action)\r\n",
        "        policy.rewards.Add(reward)\r\n",
        "        env.render()\r\n",
        "        if done:\r\n",
        "          break\r\n",
        "        loss = train (policy, optimizer)\r\n",
        "        total_reward = sum(policy.rewards)\r\n",
        "        solved = total_reward> TERMINATION\r\n",
        "        policy.onpolicy_reset()\r\n",
        "        print(f'Episode: {epi}, loss: {loss}, total reward:{total_reward}, solved: {solved}')\r\n",
        "\r\n",
        "  \r\n",
        "  if __name__ == '__main__':\r\n",
        "    main()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}